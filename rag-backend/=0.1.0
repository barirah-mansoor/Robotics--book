Defaulting to user installation because normal site-packages is not writeable
Collecting fastapi==0.104.1
  Using cached fastapi-0.104.1-py3-none-any.whl.metadata (24 kB)
Collecting uvicorn==0.24.0.post1
  Using cached uvicorn-0.24.0.post1-py3-none-any.whl.metadata (6.4 kB)
Collecting python-dotenv==1.0.0
  Using cached python_dotenv-1.0.0-py3-none-any.whl.metadata (21 kB)
Collecting qdrant-client
  Using cached qdrant_client-1.16.1-py3-none-any.whl.metadata (11 kB)
Collecting llama-index
  Using cached llama_index-0.14.10-py3-none-any.whl.metadata (13 kB)
Collecting llama-index-llms-openai
  Using cached llama_index_llms_openai-0.6.10-py3-none-any.whl.metadata (3.0 kB)
Collecting llama-index-embeddings-openai
  Using cached llama_index_embeddings_openai-0.5.1-py3-none-any.whl.metadata (400 bytes)
Collecting llama-index-vector-stores-qdrant
  Using cached llama_index_vector_stores_qdrant-0.1.4-py3-none-any.whl.metadata (696 bytes)
Collecting anyio<4.0.0,>=3.7.1 (from fastapi==0.104.1)
  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)
Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\users\admin\appdata\roaming\python\python314\site-packages (from fastapi==0.104.1) (2.12.4)
Collecting starlette<0.28.0,>=0.27.0 (from fastapi==0.104.1)
  Using cached starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)
Requirement already satisfied: typing-extensions>=4.8.0 in c:\python314\lib\site-packages (from fastapi==0.104.1) (4.15.0)
Requirement already satisfied: click>=7.0 in c:\python314\lib\site-packages (from uvicorn==0.24.0.post1) (8.3.1)
Requirement already satisfied: h11>=0.8 in c:\python314\lib\site-packages (from uvicorn==0.24.0.post1) (0.16.0)
Requirement already satisfied: idna>=2.8 in c:\python314\lib\site-packages (from anyio<4.0.0,>=3.7.1->fastapi==0.104.1) (3.11)
Requirement already satisfied: sniffio>=1.1 in c:\users\admin\appdata\roaming\python\python314\site-packages (from anyio<4.0.0,>=3.7.1->fastapi==0.104.1) (1.3.1)
Requirement already satisfied: annotated-types>=0.6.0 in c:\users\admin\appdata\roaming\python\python314\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.104.1) (0.7.0)
Requirement already satisfied: pydantic-core==2.41.5 in c:\users\admin\appdata\roaming\python\python314\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.104.1) (2.41.5)
Requirement already satisfied: typing-inspection>=0.4.2 in c:\users\admin\appdata\roaming\python\python314\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.104.1) (0.4.2)
Collecting grpcio>=1.41.0 (from qdrant-client)
  Using cached grpcio-1.76.0-cp314-cp314-win_amd64.whl.metadata (3.8 kB)
Requirement already satisfied: httpx>=0.20.0 in c:\python314\lib\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)
Requirement already satisfied: numpy>=2.1.0 in c:\users\admin\appdata\roaming\python\python314\site-packages (from qdrant-client) (2.3.5)
Collecting portalocker<4.0,>=2.7.0 (from qdrant-client)
  Using cached portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)
Collecting protobuf>=3.20.0 (from qdrant-client)
  Using cached protobuf-6.33.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)
Requirement already satisfied: urllib3<3,>=1.26.14 in c:\users\admin\appdata\roaming\python\python314\site-packages (from qdrant-client) (2.5.0)
Requirement already satisfied: pywin32>=226 in c:\users\admin\appdata\roaming\python\python314\site-packages (from portalocker<4.0,>=2.7.0->qdrant-client) (311)
Collecting llama-index-cli<0.6,>=0.5.0 (from llama-index)
  Using cached llama_index_cli-0.5.3-py3-none-any.whl.metadata (1.4 kB)
Collecting llama-index-core<0.15.0,>=0.14.10 (from llama-index)
  Using cached llama_index_core-0.14.10-py3-none-any.whl.metadata (2.5 kB)
Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)
  Using cached llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl.metadata (3.7 kB)
Collecting llama-index-readers-file<0.6,>=0.5.0 (from llama-index)
  Using cached llama_index_readers_file-0.5.5-py3-none-any.whl.metadata (5.7 kB)
Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)
  Using cached llama_index_readers_llama_parse-0.5.1-py3-none-any.whl.metadata (3.1 kB)
Collecting nltk>3.8.1 (from llama-index)
  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)
Collecting openai<3,>=1.108.1 (from llama-index-llms-openai)
  Using cached openai-2.9.0-py3-none-any.whl.metadata (29 kB)
Collecting aiohttp<4,>=3.8.6 (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached aiohttp-3.13.2-cp314-cp314-win_amd64.whl.metadata (8.4 kB)
Collecting aiosqlite (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)
Collecting banks<3,>=2.2.0 (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached banks-2.2.0-py3-none-any.whl.metadata (12 kB)
Collecting dataclasses-json (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)
Collecting deprecated>=1.2.9.3 (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)
Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)
Collecting filetype<2,>=1.2.0 (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)
Collecting fsspec>=2023.5.0 (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)
Collecting llama-index-workflows!=2.9.0,<3,>=2 (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached llama_index_workflows-2.11.5-py3-none-any.whl.metadata (4.7 kB)
Collecting nest-asyncio<2,>=1.5.8 (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)
Collecting networkx>=3.0 (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached networkx-3.6-py3-none-any.whl.metadata (6.8 kB)
Collecting pillow>=9.0.0 (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached pillow-12.0.0-cp314-cp314-win_amd64.whl.metadata (9.0 kB)
Requirement already satisfied: platformdirs in c:\python314\lib\site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (4.5.0)
Requirement already satisfied: pyyaml>=6.0.1 in c:\users\admin\appdata\roaming\python\python314\site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (6.0.3)
Requirement already satisfied: requests>=2.31.0 in c:\users\admin\appdata\roaming\python\python314\site-packages (from llama-index-core<0.15.0,>=0.14.10->llama-index) (2.32.5)
Collecting setuptools>=80.9.0 (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)
Requirement already satisfied: sqlalchemy>=1.4.49 in c:\users\admin\appdata\roaming\python\python314\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.10->llama-index) (2.0.44)
Collecting tenacity!=8.4.0,<10.0.0,>=8.2.0 (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)
Collecting tiktoken>=0.7.0 (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached tiktoken-0.12.0-cp314-cp314-win_amd64.whl.metadata (6.9 kB)
Collecting tqdm<5,>=4.66.1 (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)
Collecting typing-inspect>=0.8.0 (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)
Collecting wrapt (from llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached wrapt-2.0.1-cp314-cp314-win_amd64.whl.metadata (9.2 kB)
Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)
Collecting aiosignal>=1.4.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)
Requirement already satisfied: attrs>=17.3.0 in c:\users\admin\appdata\roaming\python\python314\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.10->llama-index) (25.4.0)
Collecting frozenlist>=1.1.1 (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached frozenlist-1.8.0-cp314-cp314-win_amd64.whl.metadata (21 kB)
Collecting multidict<7.0,>=4.5 (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached multidict-6.7.0-cp314-cp314-win_amd64.whl.metadata (5.5 kB)
Collecting propcache>=0.2.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached propcache-0.4.1-cp314-cp314-win_amd64.whl.metadata (14 kB)
Collecting yarl<2.0,>=1.17.0 (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached yarl-1.22.0-cp314-cp314-win_amd64.whl.metadata (77 kB)
Collecting griffe (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached griffe-1.15.0-py3-none-any.whl.metadata (5.2 kB)
Collecting jinja2 (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting beautifulsoup4<5,>=4.12.3 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)
  Using cached beautifulsoup4-4.14.3-py3-none-any.whl.metadata (3.8 kB)
Collecting defusedxml>=0.7.1 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)
  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)
Requirement already satisfied: pandas<2.3.0 in c:\users\admin\appdata\roaming\python\python314\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.3)
Collecting pypdf<7,>=6.1.3 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)
  Using cached pypdf-6.4.0-py3-none-any.whl.metadata (7.1 kB)
Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)
  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)
Collecting soupsieve>=1.6.1 (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index)
  Using cached soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)
Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.10->llama-index)
  Using cached llama_index_instrumentation-0.4.2-py3-none-any.whl.metadata (1.1 kB)
Collecting distro<2,>=1.7.0 (from openai<3,>=1.108.1->llama-index-llms-openai)
  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)
Collecting jiter<1,>=0.10.0 (from openai<3,>=1.108.1->llama-index-llms-openai)
  Using cached jiter-0.12.0-cp314-cp314-win_amd64.whl.metadata (5.3 kB)
Requirement already satisfied: certifi in c:\python314\lib\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.11.12)
Requirement already satisfied: httpcore==1.* in c:\python314\lib\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)
Requirement already satisfied: python-dateutil>=2.8.2 in c:\users\admin\appdata\roaming\python\python314\site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in c:\users\admin\appdata\roaming\python\python314\site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in c:\users\admin\appdata\roaming\python\python314\site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)
Requirement already satisfied: colorama in c:\python314\lib\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.15.0,>=0.14.10->llama-index) (0.4.6)
INFO: pip is looking at multiple versions of llama-index-vector-stores-qdrant to determine which version is compatible with other requirements. This could take a while.
Collecting llama-index-vector-stores-qdrant
  Using cached llama_index_vector_stores_qdrant-0.1.3-py3-none-any.whl.metadata (749 bytes)
Collecting llama-index-readers-file<0.6,>=0.5.0 (from llama-index)
  Using cached llama_index_readers_file-0.5.4-py3-none-any.whl.metadata (5.7 kB)
  Using cached llama_index_readers_file-0.5.3-py3-none-any.whl.metadata (5.7 kB)
INFO: pip is looking at multiple versions of llama-index-readers-file to determine which version is compatible with other requirements. This could take a while.
  Using cached llama_index_readers_file-0.5.2-py3-none-any.whl.metadata (5.7 kB)
  Using cached llama_index_readers_file-0.5.1-py3-none-any.whl.metadata (5.7 kB)
  Using cached llama_index_readers_file-0.5.0-py3-none-any.whl.metadata (5.3 kB)
Collecting llama-index-cli<0.6,>=0.5.0 (from llama-index)
  Using cached llama_index_cli-0.5.2-py3-none-any.whl.metadata (1.4 kB)
INFO: pip is still looking at multiple versions of llama-index-vector-stores-qdrant to determine which version is compatible with other requirements. This could take a while.
INFO: pip is still looking at multiple versions of llama-index-readers-file to determine which version is compatible with other requirements. This could take a while.
  Using cached llama_index_cli-0.5.1-py3-none-any.whl.metadata (1.4 kB)
INFO: pip is looking at multiple versions of llama-index-cli to determine which version is compatible with other requirements. This could take a while.
  Using cached llama_index_cli-0.5.0-py3-none-any.whl.metadata (1.4 kB)
Collecting llama-index-embeddings-openai
  Using cached llama_index_embeddings_openai-0.5.0-py3-none-any.whl.metadata (400 bytes)
INFO: pip is looking at multiple versions of llama-index-embeddings-openai to determine which version is compatible with other requirements. This could take a while.
Collecting llama-index-llms-openai
  Using cached llama_index_llms_openai-0.6.9-py3-none-any.whl.metadata (3.0 kB)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
  Using cached llama_index_llms_openai-0.6.8-py3-none-any.whl.metadata (3.0 kB)
  Using cached llama_index_llms_openai-0.6.7-py3-none-any.whl.metadata (3.0 kB)
Collecting openai<2,>=1.108.1 (from llama-index-llms-openai)
  Using cached openai-1.109.1-py3-none-any.whl.metadata (29 kB)
INFO: pip is still looking at multiple versions of llama-index-cli to determine which version is compatible with other requirements. This could take a while.
Collecting llama-index-llms-openai
  Using cached llama_index_llms_openai-0.6.6-py3-none-any.whl.metadata (3.0 kB)
  Using cached llama_index_llms_openai-0.6.5-py3-none-any.whl.metadata (3.0 kB)
  Using cached llama_index_llms_openai-0.6.4-py3-none-any.whl.metadata (3.0 kB)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
  Using cached llama_index_llms_openai-0.6.3-py3-none-any.whl.metadata (3.0 kB)
INFO: pip is still looking at multiple versions of llama-index-embeddings-openai to determine which version is compatible with other requirements. This could take a while.
  Using cached llama_index_llms_openai-0.6.2-py3-none-any.whl.metadata (3.0 kB)
  Using cached llama_index_llms_openai-0.6.1-py3-none-any.whl.metadata (3.0 kB)
  Using cached llama_index_llms_openai-0.6.0-py3-none-any.whl.metadata (3.0 kB)
Collecting llama-index
  Using cached llama_index-0.14.9-py3-none-any.whl.metadata (13 kB)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
  Using cached llama_index-0.14.8-py3-none-any.whl.metadata (13 kB)
  Using cached llama_index-0.14.7-py3-none-any.whl.metadata (13 kB)
  Using cached llama_index-0.14.6-py3-none-any.whl.metadata (13 kB)
  Using cached llama_index-0.14.5-py3-none-any.whl.metadata (13 kB)
  Using cached llama_index-0.14.4-py3-none-any.whl.metadata (13 kB)
  Using cached llama_index-0.14.3-py3-none-any.whl.metadata (13 kB)
Collecting llama-index-llms-openai
  Using cached llama_index_llms_openai-0.5.6-py3-none-any.whl.metadata (3.0 kB)
  Using cached llama_index_llms_openai-0.5.5-py3-none-any.whl.metadata (3.0 kB)
INFO: pip is looking at multiple versions of llama-index-llms-openai to determine which version is compatible with other requirements. This could take a while.
  Using cached llama_index_llms_openai-0.5.4-py3-none-any.whl.metadata (3.0 kB)
  Using cached llama_index_llms_openai-0.5.3-py3-none-any.whl.metadata (3.0 kB)
  Using cached llama_index_llms_openai-0.5.2-py3-none-any.whl.metadata (3.0 kB)
  Using cached llama_index_llms_openai-0.5.1-py3-none-any.whl.metadata (3.0 kB)
  Using cached llama_index_llms_openai-0.5.0-py3-none-any.whl.metadata (3.0 kB)
Collecting llama-index
  Using cached llama_index-0.14.2-py3-none-any.whl.metadata (13 kB)
INFO: pip is still looking at multiple versions of llama-index-llms-openai to determine which version is compatible with other requirements. This could take a while.
  Using cached llama_index-0.14.1-py3-none-any.whl.metadata (13 kB)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
  Using cached llama_index-0.14.0-py3-none-any.whl.metadata (12 kB)
Collecting llama-index-core<0.15,>=0.13.6 (from llama-index)
  Using cached llama_index_core-0.13.6-py3-none-any.whl.metadata (2.5 kB)
Collecting llama-index-workflows<2,>=1.0.1 (from llama-index-core<0.15,>=0.13.6->llama-index)
  Using cached llama_index_workflows-1.3.0-py3-none-any.whl.metadata (6.4 kB)
Collecting pypdf<6,>=5.1.0 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)
  Using cached pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)
Collecting llama-index
  Using cached llama_index-0.13.6-py3-none-any.whl.metadata (12 kB)
  Using cached llama_index-0.13.5-py3-none-any.whl.metadata (12 kB)
  Using cached llama_index-0.13.4-py3-none-any.whl.metadata (12 kB)
  Using cached llama_index-0.13.3-py3-none-any.whl.metadata (12 kB)
  Using cached llama_index-0.13.2-py3-none-any.whl.metadata (12 kB)
  Using cached llama_index-0.13.1-py3-none-any.whl.metadata (12 kB)
  Using cached llama_index-0.13.0-py3-none-any.whl.metadata (12 kB)
  Using cached llama_index-0.12.52-py3-none-any.whl.metadata (12 kB)
Collecting llama-index-agent-openai<0.5,>=0.4.0 (from llama-index)
  Using cached llama_index_agent_openai-0.4.12-py3-none-any.whl.metadata (439 bytes)
Collecting llama-index-cli<0.5,>=0.4.2 (from llama-index)
  Using cached llama_index_cli-0.4.4-py3-none-any.whl.metadata (1.4 kB)
Collecting llama-index-core<0.13,>=0.12.52.post1 (from llama-index)
  Using cached llama_index_core-0.12.52.post1-py3-none-any.whl.metadata (2.5 kB)
Collecting llama-index-embeddings-openai
  Using cached llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)
Collecting llama-index-llms-openai
  Using cached llama_index_llms_openai-0.4.7-py3-none-any.whl.metadata (3.0 kB)
Collecting llama-index-multi-modal-llms-openai<0.6,>=0.5.0 (from llama-index)
  Using cached llama_index_multi_modal_llms_openai-0.5.3-py3-none-any.whl.metadata (441 bytes)
Collecting llama-index-program-openai<0.4,>=0.3.0 (from llama-index)
  Using cached llama_index_program_openai-0.3.2-py3-none-any.whl.metadata (473 bytes)
Collecting llama-index-question-gen-openai<0.4,>=0.3.0 (from llama-index)
  Using cached llama_index_question_gen_openai-0.3.1-py3-none-any.whl.metadata (492 bytes)
Collecting llama-index-readers-file<0.5,>=0.4.0 (from llama-index)
  Using cached llama_index_readers_file-0.4.11-py3-none-any.whl.metadata (5.3 kB)
  Using cached llama_index_readers_file-0.4.9-py3-none-any.whl.metadata (5.2 kB)
  Using cached llama_index_readers_file-0.4.8-py3-none-any.whl.metadata (5.2 kB)
  Using cached llama_index_readers_file-0.4.7-py3-none-any.whl.metadata (5.4 kB)
  Using cached llama_index_readers_file-0.4.6-py3-none-any.whl.metadata (5.4 kB)
  Using cached llama_index_readers_file-0.4.5-py3-none-any.whl.metadata (5.4 kB)
  Using cached llama_index_readers_file-0.4.4-py3-none-any.whl.metadata (5.4 kB)
  Using cached llama_index_readers_file-0.4.3-py3-none-any.whl.metadata (5.4 kB)
  Using cached llama_index_readers_file-0.4.2-py3-none-any.whl.metadata (5.4 kB)
  Using cached llama_index_readers_file-0.4.1-py3-none-any.whl.metadata (5.4 kB)
  Using cached llama_index_readers_file-0.4.0-py3-none-any.whl.metadata (5.4 kB)
Collecting llama-index-question-gen-openai<0.4,>=0.3.0 (from llama-index)
  Using cached llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)
INFO: pip is looking at multiple versions of llama-index-question-gen-openai to determine which version is compatible with other requirements. This could take a while.
Collecting llama-index-program-openai<0.4,>=0.3.0 (from llama-index)
  Using cached llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)
INFO: pip is looking at multiple versions of llama-index-program-openai to determine which version is compatible with other requirements. This could take a while.
  Using cached llama_index_program_openai-0.3.0-py3-none-any.whl.metadata (764 bytes)
Collecting llama-index-multi-modal-llms-openai<0.6,>=0.5.0 (from llama-index)
  Using cached llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl.metadata (440 bytes)
  Using cached llama_index_multi_modal_llms_openai-0.5.0-py3-none-any.whl.metadata (727 bytes)
INFO: pip is looking at multiple versions of llama-index-multi-modal-llms-openai to determine which version is compatible with other requirements. This could take a while.
Collecting llama-index-agent-openai<0.5,>=0.4.0 (from llama-index)
  Using cached llama_index_agent_openai-0.4.11-py3-none-any.whl.metadata (439 bytes)
INFO: pip is still looking at multiple versions of llama-index-program-openai to determine which version is compatible with other requirements. This could take a while.
  Using cached llama_index_agent_openai-0.4.10-py3-none-any.whl.metadata (439 bytes)
  Using cached llama_index_agent_openai-0.4.9-py3-none-any.whl.metadata (438 bytes)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
INFO: pip is still looking at multiple versions of llama-index-question-gen-openai to determine which version is compatible with other requirements. This could take a while.
  Using cached llama_index_agent_openai-0.4.8-py3-none-any.whl.metadata (438 bytes)
INFO: pip is looking at multiple versions of llama-index-agent-openai to determine which version is compatible with other requirements. This could take a while.
  Using cached llama_index_agent_openai-0.4.7-py3-none-any.whl.metadata (438 bytes)
  Using cached llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)
  Using cached llama_index_agent_openai-0.4.5-py3-none-any.whl.metadata (727 bytes)
  Using cached llama_index_agent_openai-0.4.4-py3-none-any.whl.metadata (727 bytes)
  Using cached llama_index_agent_openai-0.4.3-py3-none-any.whl.metadata (727 bytes)
  Using cached llama_index_agent_openai-0.4.2-py3-none-any.whl.metadata (727 bytes)
  Using cached llama_index_agent_openai-0.4.1-py3-none-any.whl.metadata (726 bytes)
INFO: pip is still looking at multiple versions of llama-index-agent-openai to determine which version is compatible with other requirements. This could take a while.
  Using cached llama_index_agent_openai-0.4.0-py3-none-any.whl.metadata (726 bytes)
Collecting llama-index-cli<0.5,>=0.4.2 (from llama-index)
  Using cached llama_index_cli-0.4.3-py3-none-any.whl.metadata (1.4 kB)
  Using cached llama_index_cli-0.4.2-py3-none-any.whl.metadata (1.6 kB)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
INFO: pip is still looking at multiple versions of llama-index-multi-modal-llms-openai to determine which version is compatible with other requirements. This could take a while.
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
Collecting llama-index
  Using cached llama_index-0.12.51-py3-none-any.whl.metadata (12 kB)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
  Using cached llama_index-0.12.50-py3-none-any.whl.metadata (12 kB)
  Using cached llama_index-0.12.49-py3-none-any.whl.metadata (12 kB)
  Using cached llama_index-0.12.48-py3-none-any.whl.metadata (12 kB)
  Using cached llama_index-0.12.47-py3-none-any.whl.metadata (12 kB)
